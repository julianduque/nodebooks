id: web-scraping
title: Web Scraping Notebook
description: >-
  Fetch HTML pages with the native Fetch API, parse content using Cheerio, and
  collect structured insights without heavyweight browser automation.
badge:
  text: Scraping
  tone: amber
tags:
  - scraping
  - cheerio
  - automation
order: 50
notebook:
  name: Web Scraping Toolkit
  env:
    runtime: node
    version: "20.x"
    packages:
      cheerio: "^1.0.0-rc.12"
      p-limit: "^5.0.0"
    variables:
      TARGET_URL: https://example.com/blog
      MAX_CONCURRENCY: "4"
  cells:
    - type: markdown
      source: |-
        # Lightweight scraping workflow

        Assemble quick scrapers with the built-in Fetch API and Cheerio for HTML
        parsing. Respect target site policies and rate limits before running any
        automated collection.
    - type: markdown
      source: |-
        ## Environment overview

        - `TARGET_URL`: starting page to crawl
        - `MAX_CONCURRENCY`: number of concurrent detail fetches when enriching items

        Dependencies:

        - [`cheerio`](https://cheerio.js.org/) for DOM parsing
        - [`p-limit`](https://github.com/sindresorhus/p-limit) for concurrency control
    - type: code
      language: ts
      source: |-
        import { load } from "cheerio";

        interface ArticleSummary {
          title: string;
          url: string;
          summary: string;
        }

        const target = process.env.TARGET_URL ?? "https://example.com/blog";
        const response = await fetch(target);
        const html = await response.text();
        const $ = load(html);

        export const articles: ArticleSummary[] = $("article")
          .map((_, el) => {
            const heading = $(el).find("h2").first();
            const href = heading.find("a").attr("href") ?? target;
            const summary = $(el).find("p").first().text().trim();
            return {
              title: heading.text().trim(),
              url: new URL(href, target).toString(),
              summary,
            };
          })
          .get();

        console.log(`Discovered ${articles.length} articles from ${target}`);
    - type: code
      language: ts
      source: |-
        import { load } from "cheerio";
        import pLimit from "p-limit";

        const concurrency = Number.parseInt(
          process.env.MAX_CONCURRENCY ?? "4",
          10
        );
        const limit = pLimit(Math.max(1, concurrency));

        export async function enrichArticles(max = 5) {
          const subset = articles.slice(0, max);
          const results = await Promise.all(
            subset.map((article) =>
              limit(async () => {
                const res = await fetch(article.url);
                const html = await res.text();
                const $ = load(html);
                const headline = $("h1, h2").first().text().trim();
                return { ...article, headline };
              })
            )
          );
          console.log("Enriched articles:", results);
          return results;
        }

        console.log(
          "Call await enrichArticles() to fetch detail pages with concurrency control."
        );
      outputs:
        - type: display_data
          data:
            'application/vnd.nodebooks.ui+json':
              ui: alert
              level: warn
              title: Be respectful
              text: Always review a site's robots.txt and terms of service before scraping.
        - type: display_data
          data:
            'application/vnd.nodebooks.ui+json':
              ui: table
              columns:
                - key: title
                  label: Sample Title
                - key: url
                  label: URL
                - key: summary
                  label: Summary
              rows:
                - title: Build faster scrapers
                  url: https://example.com/blog/scraper
                  summary: Queue-friendly techniques for efficient scraping.
                - title: Respectful crawling
                  url: https://example.com/blog/ethics
                  summary: Guidelines for staying within terms of service.
